{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA INGESTION PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'talk.txt'}, page_content='I am happy to join with you today in what will go down in history as the greatest demonstration for\\nfreedom in the history of our nation.\\nFive score years ago a great American in whose symbolic shadow we stand today signed the\\nEmancipation Proclamation. This momentous decree is a great beacon light of hope to millions of Negro\\nslaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the\\nlong night of their captivity. But 100 years later the Negro still is not free. One hundred years later the\\nlife of the Negro is still badly crippled by the manacles of segregation and the chains of discrimination.\\nOne hundred years later the Negro lives on a lonely island of poverty in the midst of a vast ocean of\\nmaterial prosperity. One hundred years later the Negro is still languished in the corners of American\\nsociety and finds himself in exile in his own land. So we’ve come here today to dramatize a shameful\\ncondition.\\nIn a sense we’ve come to our nation’s capital to cash a check. When the architects of our Republic wrote\\nthe magnificent words of the Constitution and the Declaration of Independence, they were signing a\\npromissory note to which every American was to fall heir. This note was a promise that all men—yes,\\nblack men as well as white men—would be guaranteed the unalienable rights of life, liberty and the\\npursuit of happiness. . . .\\nWe must forever conduct our struggle on the high plane of dignity and discipline. We must not allow our\\ncreative protests to degenerate into physical violence. . . . The marvelous new militancy which has\\nengulfed the Negro community must not lead us to distrust all white people, for many of our white\\nbrothers, as evidenced by their presence here today, have come to realize that their destiny is tied up\\nwith our destiny.\\n. . . We cannot walk alone. And as we walk we must make the pledge that we shall always march ahead.\\nWe cannot turn back. There are those who are asking the devotees of civil rights, “When will you be\\nsatisfied?” We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of\\npolice brutality.\\nWe can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in\\nthe motels of the highways and the hotels of the cities.\\nWe cannot be satisfied as long as the Negro’s basic mobility is from a smaller ghetto to a larger one. We\\ncan never be satisfied as long as our children are stripped of their adulthood and robbed of their dignity\\nby signs stating “For Whites Only.”\\nWe cannot be satisfied as long as the Negro in Mississippi cannot vote and the Negro in New York\\nbelieves he has nothing for which to vote.\\nNo, no, we are not satisfied, and we will not be satisfied until justice rolls down like waters and\\nrighteousness like a mighty stream. . . .\\nI say to you today, my friends, though, even though we face the difficulties of today and tomorrow, I still \\nhave a dream. It is a dream deeply rooted in the American dream. I have a dream that one day this\\nnation will rise up, live out the true meaning of its creed: “We hold these truths to be self-evident, that\\nall men are created equal.”\\nI have a dream that one day on the red hills of Georgia sons of former slaves and the sons of former\\nslave-owners will be able to sit down together at the table of brotherhood. I have a dream that one day\\neven the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of\\noppression, will be transformed into an oasis of freedom and justice.\\nI have a dream that my four little children will one day live in a nation where they will not be judged by\\nthe color of their skin but by the content of their character. I have a dream . . . I have a dream that one\\nday in Alabama, with its vicious racists, with its governor having his lips dripping with the words of\\ninterposition and nullification, one day right there in Alabama little black boys and black girls will be able\\nto join hands with little white boys and white girls as sisters and brothers.\\nI have a dream today . . .\\nThis will be the day when all of God’s children will be able to sing with new meaning. “My country, ’tis of\\nthee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrim’s pride, from\\nevery mountain side, let freedom ring.” And if America is to be a great nation, this must become true. So\\nlet freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty\\nmountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let\\nfreedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of\\nCalifornia.\\nBut not only that. Let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout\\nMountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi, from every\\nmountain side. Let freedom ring . . .\\nWhen we allow freedom to ring—when we let it ring from every city and every hamlet, from every state\\nand every city, we will be able to speed up that day when all of God’s children, black men and white\\nmen, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the\\nold Negro spiritual, “Free at last, Free at last, Great God a-mighty, We are free at last.”')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"talk.txt\")\n",
    "\n",
    "text_from_txtfile=loader.load()\n",
    "\n",
    "text_from_txtfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "\n",
    "loader2 = WebBaseLoader(\n",
    "    web_path=(\"https://en.wikipedia.org/wiki/Martin_Luther_King_Jr.\",),\n",
    "    bs_kwargs={\n",
    "        'parse_only': bs4.SoupStrainer(\n",
    "            class_=(\"mw-page-container\")\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "txt_from_url=loader2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'paper2.pdf', 'page': 0}, page_content='American Sign Language Alphabet Recognition  using \\nDeep Learning  \\nNikhil Kasukurthi1 Brij Rokad2 Shiv Bidani3 Aju D ennisan4 \\n{ 1nikhil.kasukurthi, 2brij.rokad, 3shivbidani  }@gmail.com    |   4daju@vit.ac.in           \\nVIT University  \\n \\nAbstract - Tremendous headway has been made in the field of 3D hand pose estimation but the 3D depth cameras are \\nusually inaccessible. We propose a model to recognize  American Sign Language alphabet from RGB images. Images \\nfor the training were resized and pre -processed before training the Deep Neural Network. The model was trained on \\na squeezenet architecture to make it capable of running on mobile devices with an acc uracy of 83.29%.   \\n \\nKeywords: Sign Language detection; Squeezenet; Deep Neural Network; Stochastic Gradient Descent  \\n \\n1. Introduction  \\nSign language conversion has  been a long standing computer vision problem[1]. Several solutions have come up but none of \\nthem have been portable for them to be used in a standalone device or application. We plan on alleviating this problem by \\nharnessing the power of the mobile phone and the recent advances in deep learning.  \\n  \\nWith the advent of deep learning, end to end models are being built for a wide range of problems that only require the \\nimages as input. Datasets have made it possible to harness the power of the models better. Im agenet is the best example, it is \\nstill driving innovation and advancements in computer vision. Another such dataset is the Microsoft Coco which is one of \\nthe benchmarks for image segmentation and human pose estimation.  The problem of image classification  has become very \\ntrivial now, on the other hand image segmentation is still quite difficult.  \\n  \\nWe propose an end to end solution that would require only a 2D image as an input. For this we follow a 3 Segment approach \\ninspired by [2]. Our aim is to make it easy for people to communicate using the model. There are numerous people who use \\nthe ASL around the world. A vision based approach to our solution attempts to reduce the requirement of human translators \\nand increase dependency on the user’s phone for tran slation.  \\n  \\n2. Related Work  \\nThere have been multiple instances where the accuracy of the Hidden Markov Model approaches 94% [3]. The HMM model \\nutilizes  2 cameras. One camera is mounted on the desk  (92%) and the other is a wearable cap with an attached camer a \\n(94%). The accuracy increases when the position of camera is changed. This shows that perspective plays an important role \\nin the determination of the accuracy.  \\n  \\nIn [4] the system utilizes  a PCA analyzer  model to determine the position of the hand shape.  It also uses a motion chain code \\nto understand the hand movement to make determinations about signs that require movement. They have managed to achieve \\nan error rate of 10.91%.  \\n  \\n[5] employs two models, one an effective hierarchical feature characterizati on scheme and the other is a TMDHMM network \\nto fasten the recognition process without any loss in recognition accuracy. In [6] there is a three -stage  model that relies on \\nmovement of the hand. In the first phase the image of the hand is captured in a sequence. The sequence is evaluated to \\ndetermine the events. These events determine the complete start to end positions.  The second phase is the segmentat ion of \\nthe image and includes the training based on the contours of the hand. The final phase is the determination phase where the \\nimage is provided with a score to determine if it belongs to the signs that have been learned.  \\n  \\nWhile [7] employs a wearable  glove that is used on the hand with colors  that assist the camera to determine the tips of the \\nfingers as they are far apart on the color  spectrum. The process involves reading the frame, segmenting the color , finding the \\ncentroid of the image and the fin al step is the determination of the result.  \\n  '),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 1}, page_content='[8] has utilized a number of parameters including posture of the hand, position, motion and orientation. The model uses a \\ntypical HMM with a sub -par accuracy level due to the processing happening for 51 posture s, 6 orientations and 6 positions. \\nThe accuracy obtained is around 80.4%.  \\n  \\nNeural nets and Hough Transform have been used for ASL detection in certain models [9]. It utilizes a vector feature \\nfunction for comparison. The vector feature is not prone to any  disturbances due to rotation and scaling, this enables the \\nsystem to be highly flexible and robust. The accuracy attained by this methodology was around 92% which shows that it is \\nan industry standard solution to the ASL detection problem.  \\n \\n  \\nFig. 1 ASL  alphabet. Top Left - A \\n  \\nThere are multiple other researchers that have aimed to solve the ASL detection problem with accuracies ranging from 70 -\\n94%.  \\n \\nWhile in [11] communication via gestures (ISL) comprises of static and in addition dynamic hand motions . A large portion \\nof the ISL signals are created utilizing the two hands. A video database is made and used which contains a few recordings, \\nfor an expansive number of signs. Bearing histogram is the component utilized for arrangement because of its allure  for \\nbrightening and introduction invariance. Two distinctive methodologies used for acknowledgment are Euclidean separation \\nand K -closest neighbor measurements.  \\n \\n[12] Exhibits the assessment of different pixel level highlights for the dual handed sign lan guage dataset. The element \\nextraction techniques are Histogram of Orientation Gradient (HOG), Histogram of Boundary Description (HBD) and the \\nHistogram of Edge Frequency (HOEF). The exactness of HOG and HBD found up to 71.4% and 77.3% while the precision \\nof HOEF, all things considered, informational collection is 97.3% and in perfect condition 98.1%.  \\n \\nIn [13] a wavelet based video division procedure is proposed which identifies states of different hand signs and head \\ndevelopment in video based setup. Shape highlights of hand motions are extricated using elliptical Fourier descriptions \\nwhich to the most elevated degree lessens the element vectors for a picture. The exploratory outcomes demonstrate that \\nframework has an accuracy rate of 96%.  \\n \\n[14] Horn Schunck  optical stream calculation removes highlights of the two hands giving position vectors of hands in each \\nedge. The joined component framework having shape highlights prepare the back -propagation neural network. The ordered \\nsigns are mapped to content from the objective grid of ANN and changing over those content contributions to voice charges \\nwith windows content to application programmable interface. The word coordinating score over various occurrences of \\npreparing and testing of the neural network brought  around 90%.  \\n'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 2}, page_content='  \\n \\n \\n \\nTable 1. Comparison Table for Various SL Detection Models  \\nAuthor  Model / Method  Input  Feature Vector  Classification  Accuracy  \\nNandy, A., Prasad , \\net al[11]  HMM  Real time \\nvideo  Orientation \\nHistogram  Euclidean distance  90% \\nLilha, H., & \\nShivmurthy, D [12] Histogram of Orientation \\nGradient (HOG), Histogram \\nof Boundary Description \\n(HBD) and the Histogram of \\nEdge Frequency (HOEF)  Images  Histogram of Edge \\nFrequency (HOEF).  Support Vector \\nMachine  98.1%  \\nKishore, P. V. V., & \\nKumar, P. R [13] PCA  Video  Elliptical Fourier \\ndescriptors  Fuzzy  91% \\nKishore, P. V. V., \\nPrasad, M. V. D. , et \\nal[14]  HMM  Real-time Texture features  ANN - error back \\npropagation algorithm  91% \\n  \\n3. Methodology  \\n \\n \\nFig. 2 Proposed Architecture  \\n \\n \\n \\nFor translating the image to the relevant alphabet,  we have  trained the pre -trained model on the SqueezeNet model. Our \\nmodel is  trained on the Surrey Finger [10] . The trained model is then used for inference from the images that are being fed as \\ninput to the image. In accordance to the pre-trained model we process the image by dividing every pixel by 255 and then \\nresizing the image to 244X244.  \\n  \\nWe have  remove d the top layers of the original network and add two dense layers followed by a softmax function that \\ngenerated the output probabi lities. A Stochastic Gradient Descent optimizer is used to ensure that the weight updates are not \\ntoo abrupt and remain similar to the original pre -trained model. The loss function that we are using is the categorical cross \\nentropy which takes in a loss ov er the target label and entire set of labels.  \\n \\n'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 3}, page_content='The cross -entropy  function  is used to determine  the loss incurred  by the network  and is given  by: \\n \\nL(θ)=−1\\nn∑[ 𝑦𝑖log(𝑝𝑖) +(1−𝑦𝑖)log (1 −𝑝𝑖)]n\\ni=−1\\nn∑∑𝑦𝑖𝑗log (𝑝𝑖𝑗)m\\njn\\ni \\n \\n \\n3.1 Dataset  \\nThe surrey finger dataset [10] has been used to train our model.  It contains 41258 training and 2728 testing samples. Each \\nsample provides RGB image (320x320 pixels), Depth map (320x320 pixels), Segmentation masks (320x320 pixels) for the \\nclasses: backgro und, person, three classes for each finger and one for each palm, 21 Key points  for each hand with their uv \\ncoordinates in the image frame, xyz coordinates in the world frame and a visibility indicator and Intrinsic Camera Matrix K. \\nFor our scenario, we us e only the RGB images.  \\n  \\n3.2 Image Preprocessing  \\nTo match the original training of the pretrained model of squeezenet, the mean value pixels are subtracted from all the \\nimages. Then resize the image to 244X244, to create more training data augmentation was  applied. The data was shuffled in \\norder to have a diverse sub dataset when picked randomly.  \\n  \\n3.3 Squeezenet  \\nThe architecture that aim to follow is the Squeezenet architecture[15]. The squeezenet architecture comprises of a number of \\nfilters. The first level comprises of four 1x1 filters that are concatenated at the next layer. The concatenation ensures that the \\nnumber of parameters is minimal. The primary objective of the squeezenet architecture is to reduce the number of \\nparameters, and in turn the siz e of the network.  \\n \\n  \\n \\n \\n \\n \\nThe concatenated layer is fed onto the expand layer and hence the number of interconnections between the squeeze layer and \\nthe expand layer are minimal. This ensures that the size of the network is low. The expand layer comprises of 3x3 filters \\nalong with more 1x1 filters. These are concatenated in order to attain the result.  \\n \\n3.4 Testing  \\nThe weights of the trained model are updated through training it over multiple epochs. The weights and biases of the \\nnetwork are trained over the epochs to determine a final network. This net is used to evaluate the input image. The \\nevaluation is a low computation process and can be carried out on a handheld mobile device.  \\n   \\n \\nFig. 3 Squeezenet Architecture  \\n '),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 4}, page_content='4. Experimental Results  \\n \\nTable 2. Test Images and Prediction  \\n \\n \\n \\n \\n \\nTest \\nImages  \\n    \\nLabel  b a t a \\nPrediction  b a a a \\n \\n \\nThe model was trained on NVIDIA K80 GPU for a dataset size of 41,258 images. The model was trained for 10 epochs . The \\ninitial training accuracy and validation accuracy increases drastically till the 5th epoch. The accuracy then attains a plateau \\nlimit as the epochs increase. The maximum validation accuracy attained is 83.29% at the 9th epoch. Whereas the maximum \\ntraining accuracy attained is 87.47%. The correlat ion between the training and validation accuracy is 98.47% which signifies \\nthat the model has been trained accurately.  \\n  \\n  \\nFig. 5 Model Training Accuracy  \\n \\nIn table 2 we show some of our results. The model is able to give accurate predictions but there are  certain cases when it \\nfails. From our observation, we noticed that this happens when similar looking alphabet like ‘a’ and ‘t’ where the difference  \\nbetween them is thumb on the side for ‘a’ whereas ‘t’ has thumb in between index and middle finger. When an  image with \\ndifferent light conditions is given, or the fingers are not visible then it leads to a false prediction.  \\n \\n5. Conclusion  \\nFrom the tests a maximum training accuracy of 87.47% is attained. The validation accuracy attained is 83.29%. The network \\nlearned the ASL that allowed it to predict the sign language in real time. The model developed was a squeezenet architecture \\nwhich enabled the complete architecture to be stored on a mobile device. This helped with the accessibility of such a solutio n \\nfor the public. Hence, algorithmic recognition for mobile devices is currently preferred in order to present the majority of \\npeople with a highly accessible solution. In the future, the dataset preprocessing will help to improve the accuracy of the \\nmodel. The lighting conditions and distance of the image from the camera should not affect the outcome of the prediction.  \\n \\n'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 5}, page_content='6. References  \\n1.Vogler, C., & Metaxas, D. (2001). A framework for recognizing the simultaneous aspects of american sign language. \\nComputer Visio n and Image Understanding , 81(3), 358 -384. \\n2. Garcia, B., & Viesca, S. A. Real -time American Sign Language Recognition with Convolutional Neural Networks.  \\n3. Starner, T., Weaver, J., & Pentland, A. (1998). Real -time american sign language recognition using desk and wearable \\ncomputer based video. IEEE Transactions on Pattern Analysis and Machine Intelligence , 20(12), 1371 -1375.  \\n4. Starner, T. E. (1995). Visua l Recognition of American Sign Language Using Hidden Markov Models . \\nMASSACHUSETTS INST OF TECH CAMBRIDGE DEPT OF BRAIN AND COGNITIVE SCIENCES.  \\n5.Zhang, L. G., Chen, Y., Fang, G., Chen, X., & Gao, W. (2004, October). A vision -based sign language recognition  system \\nusing tied -mixture density HMM. In Proceedings of the 6th international conference on Multimodal interfaces  (pp. 198 -\\n204). ACM.  \\n6.Cui, Y., & Weng, J. (2000). Appearance -based hand sign recognition from intensity image sequences. Computer Vision \\nand Image Understanding , 78(2), 157 -176. \\n7.Akmeliawati, R., Ooi, M. P. L., & Kuang, Y. C. (2007, May). Real -time Malaysian sign language translation using colour \\nsegmentation and neural network. In Instrumentation and Measurement Technology Conference Proceed ings, 2007. IMTC \\n2007. IEEE  (pp. 1 -6). IEEE.  \\n8. Liang, R. H., & Ouhyoung, M. (1998, April). A real -time continuous gesture recognition system for sign language. In \\nAutomatic Face and Gesture Recognition, 1998. Proceedings. Third IEEE International Conferen ce on  (pp. 558 -567). IEEE.  \\n9. Munib, Q., Habeeb, M., Takruri, B., & Al -Malik, H. A. (2007). American sign language (ASL) recognition based on \\nHough transform and neural networks. Expert systems with Applications , 32(1), 24 -37. \\n10. Zimmermann, C., & Brox, T . (2017). Learning to Estimate 3D Hand Pose from Single RGB Images. arXiv preprint \\narXiv:1705.01389 . \\n11. Nandy, A., Prasad, J. S., Mondal, S., Chakraborty, P., & Nandi, G. C. (2010). Recognition of isolated indian sign \\nlanguage gesture in real time. Inform ation Processing and Management , 102 -107. \\n12. Lilha, H., & Shivmurthy, D. (2011, December). Analysis of pixel level features in recognition of real life dual -handed \\nsign language data set. In Recent Trends in Information Systems (ReTIS), 2011 International  Conference on  (pp. 246 -251). \\nIEEE.  \\n13. Kishore, P. V. V., & Kumar, P. R. (2012). A video based Indian Sign Language Recognition System (INSLR) using \\nwavelet transform and fuzzy logic. International Journal of Engineering and Technology , 4(5), 537.  \\n14. Kis hore, P. V. V., Prasad, M. V. D., Kumar, D. A., & Sastry, A.S.C.S. (2016, February). Optical flow hand tracking and \\nactive contour hand shape features for continuous sign language recognition with artificial neural networks. In Advanced \\nComputing (IACC), 2 016 IEEE 6th International Conference on  (pp. 346 -351). IEEE.  \\n15. Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., & Keutzer, K. (2016). SqueezeNet: AlexNet -level \\naccuracy with 50x fewer parameters and< 0.5 MB model size. arXiv preprint  arXiv:1602.07360 . ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader3=PyPDFLoader(\"paper2.pdf\")\n",
    "\n",
    "text_from_pdf=loader3.load()\n",
    "text_from_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transforming data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'paper2.pdf', 'page': 0}, page_content='American Sign Language Alphabet Recognition  using \\nDeep Learning  \\nNikhil Kasukurthi1 Brij Rokad2 Shiv Bidani3 Aju D ennisan4 \\n{ 1nikhil.kasukurthi, 2brij.rokad, 3shivbidani  }@gmail.com    |   4daju@vit.ac.in           \\nVIT University  \\n \\nAbstract - Tremendous headway has been made in the field of 3D hand pose estimation but the 3D depth cameras are \\nusually inaccessible. We propose a model to recognize  American Sign Language alphabet from RGB images. Images \\nfor the training were resized and pre -processed before training the Deep Neural Network. The model was trained on \\na squeezenet architecture to make it capable of running on mobile devices with an acc uracy of 83.29%.   \\n \\nKeywords: Sign Language detection; Squeezenet; Deep Neural Network; Stochastic Gradient Descent  \\n \\n1. Introduction  \\nSign language conversion has  been a long standing computer vision problem[1]. Several solutions have come up but none of'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 0}, page_content='1. Introduction  \\nSign language conversion has  been a long standing computer vision problem[1]. Several solutions have come up but none of \\nthem have been portable for them to be used in a standalone device or application. We plan on alleviating this problem by \\nharnessing the power of the mobile phone and the recent advances in deep learning.  \\n  \\nWith the advent of deep learning, end to end models are being built for a wide range of problems that only require the \\nimages as input. Datasets have made it possible to harness the power of the models better. Im agenet is the best example, it is \\nstill driving innovation and advancements in computer vision. Another such dataset is the Microsoft Coco which is one of \\nthe benchmarks for image segmentation and human pose estimation.  The problem of image classification  has become very \\ntrivial now, on the other hand image segmentation is still quite difficult.'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 0}, page_content='trivial now, on the other hand image segmentation is still quite difficult.  \\n  \\nWe propose an end to end solution that would require only a 2D image as an input. For this we follow a 3 Segment approach \\ninspired by [2]. Our aim is to make it easy for people to communicate using the model. There are numerous people who use \\nthe ASL around the world. A vision based approach to our solution attempts to reduce the requirement of human translators \\nand increase dependency on the user’s phone for tran slation.  \\n  \\n2. Related Work  \\nThere have been multiple instances where the accuracy of the Hidden Markov Model approaches 94% [3]. The HMM model \\nutilizes  2 cameras. One camera is mounted on the desk  (92%) and the other is a wearable cap with an attached camer a \\n(94%). The accuracy increases when the position of camera is changed. This shows that perspective plays an important role \\nin the determination of the accuracy.'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 0}, page_content='(94%). The accuracy increases when the position of camera is changed. This shows that perspective plays an important role \\nin the determination of the accuracy.  \\n  \\nIn [4] the system utilizes  a PCA analyzer  model to determine the position of the hand shape.  It also uses a motion chain code \\nto understand the hand movement to make determinations about signs that require movement. They have managed to achieve \\nan error rate of 10.91%.  \\n  \\n[5] employs two models, one an effective hierarchical feature characterizati on scheme and the other is a TMDHMM network \\nto fasten the recognition process without any loss in recognition accuracy. In [6] there is a three -stage  model that relies on \\nmovement of the hand. In the first phase the image of the hand is captured in a sequence. The sequence is evaluated to \\ndetermine the events. These events determine the complete start to end positions.  The second phase is the segmentat ion of'),\n",
       " Document(metadata={'source': 'paper2.pdf', 'page': 0}, page_content='determine the events. These events determine the complete start to end positions.  The second phase is the segmentat ion of \\nthe image and includes the training based on the contours of the hand. The final phase is the determination phase where the \\nimage is provided with a score to determine if it belongs to the signs that have been learned.  \\n  \\nWhile [7] employs a wearable  glove that is used on the hand with colors  that assist the camera to determine the tips of the \\nfingers as they are far apart on the color  spectrum. The process involves reading the frame, segmenting the color , finding the \\ncentroid of the image and the fin al step is the determination of the result.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "txt_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200) #dividing the documents to smaller chunks\n",
    "\n",
    "documents=txt_splitter.split_documents(text_from_pdf)\n",
    "\n",
    "documents[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will convert chunks to vectors\n",
    "\n",
    "#vector embeddings and vector store\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# we need to store in the vectorstore so we will use croma db\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db=Chroma.from_documents(documents,OllamaEmbeddings(model=\"llama3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"who are the authors of American Sign Language Alphabet Recognition using Deep Learning\"\n",
    "\n",
    "result=db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 3, 'source': 'paper2.pdf'}, page_content='parameters, and in turn the siz e of the network.  \\n \\n  \\n \\n \\n \\n \\nThe concatenated layer is fed onto the expand layer and hence the number of interconnections between the squeeze layer and \\nthe expand layer are minimal. This ensures that the size of the network is low. The expand layer comprises of 3x3 filters \\nalong with more 1x1 filters. These are concatenated in order to attain the result.  \\n \\n3.4 Testing  \\nThe weights of the trained model are updated through training it over multiple epochs. The weights and biases of the \\nnetwork are trained over the epochs to determine a final network. This net is used to evaluate the input image. The \\nevaluation is a low computation process and can be carried out on a handheld mobile device.  \\n   \\n \\nFig. 3 Squeezenet Architecture'),\n",
       " Document(metadata={'page': 1, 'source': 'paper2.pdf'}, page_content='[8] has utilized a number of parameters including posture of the hand, position, motion and orientation. The model uses a \\ntypical HMM with a sub -par accuracy level due to the processing happening for 51 posture s, 6 orientations and 6 positions. \\nThe accuracy obtained is around 80.4%.  \\n  \\nNeural nets and Hough Transform have been used for ASL detection in certain models [9]. It utilizes a vector feature \\nfunction for comparison. The vector feature is not prone to any  disturbances due to rotation and scaling, this enables the \\nsystem to be highly flexible and robust. The accuracy attained by this methodology was around 92% which shows that it is \\nan industry standard solution to the ASL detection problem.  \\n \\n  \\nFig. 1 ASL  alphabet. Top Left - A \\n  \\nThere are multiple other researchers that have aimed to solve the ASL detection problem with accuracies ranging from 70 -\\n94%.'),\n",
       " Document(metadata={'page': 5, 'source': 'paper2.pdf'}, page_content='IEEE.  \\n13. Kishore, P. V. V., & Kumar, P. R. (2012). A video based Indian Sign Language Recognition System (INSLR) using \\nwavelet transform and fuzzy logic. International Journal of Engineering and Technology , 4(5), 537.  \\n14. Kis hore, P. V. V., Prasad, M. V. D., Kumar, D. A., & Sastry, A.S.C.S. (2016, February). Optical flow hand tracking and \\nactive contour hand shape features for continuous sign language recognition with artificial neural networks. In Advanced \\nComputing (IACC), 2 016 IEEE 6th International Conference on  (pp. 346 -351). IEEE.  \\n15. Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., & Keutzer, K. (2016). SqueezeNet: AlexNet -level \\naccuracy with 50x fewer parameters and< 0.5 MB model size. arXiv preprint  arXiv:1602.07360 .'),\n",
       " Document(metadata={'page': 5, 'source': 'paper2.pdf'}, page_content='6. References  \\n1.Vogler, C., & Metaxas, D. (2001). A framework for recognizing the simultaneous aspects of american sign language. \\nComputer Visio n and Image Understanding , 81(3), 358 -384. \\n2. Garcia, B., & Viesca, S. A. Real -time American Sign Language Recognition with Convolutional Neural Networks.  \\n3. Starner, T., Weaver, J., & Pentland, A. (1998). Real -time american sign language recognition using desk and wearable \\ncomputer based video. IEEE Transactions on Pattern Analysis and Machine Intelligence , 20(12), 1371 -1375.  \\n4. Starner, T. E. (1995). Visua l Recognition of American Sign Language Using Hidden Markov Models . \\nMASSACHUSETTS INST OF TECH CAMBRIDGE DEPT OF BRAIN AND COGNITIVE SCIENCES.  \\n5.Zhang, L. G., Chen, Y., Fang, G., Chen, X., & Gao, W. (2004, October). A vision -based sign language recognition  system \\nusing tied -mixture density HMM. In Proceedings of the 6th international conference on Multimodal interfaces  (pp. 198 -\\n204). ACM.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
